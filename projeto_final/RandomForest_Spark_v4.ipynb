{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importando as dependecias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "from pyspark.sql.types import StringType\n",
    "import random\n",
    "from math import log, sqrt\n",
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Função que retorna um RDD a partir de um CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def csv_to_rdd(filepath):\n",
    "    rdd = sc.textFile(filepath).mapPartitions(lambda line: csv.reader(line, delimiter=','))\n",
    "    rdd_2 = rdd.map(lambda row: (int(row[0]), row[1], row[2], int(row[3]), row[4],row[5], row[6], row[7], row[8], int(row[9]),int(row[10]), int(row[11]), row[12], row[13]))\n",
    "    rdd_3 = rdd_2.map(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "    return rdd_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def choose_random_features(row):\n",
    "    num_features = len(row) - 1\n",
    "    return random.sample(range(num_features), int(sqrt(num_features)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_features_subset(row, features_indexes):\n",
    "    return [row[i] for i in features_indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unique_counts(rows):\n",
    "    results = {}\n",
    "    for row in rows:\n",
    "        r = row[len(row) - 1]\n",
    "        if r not in results:\n",
    "            results[r] = 0\n",
    "        results[r] += 1\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def entropy(rows):\n",
    "    log2 = lambda x: log(x) / log(2)\n",
    "    results = unique_counts(rows)\n",
    "    ent = 0.0\n",
    "    for r in results.keys():\n",
    "        p = float(results[r]) / len(rows)\n",
    "        ent = ent - p * log2(p)\n",
    "    return ent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def divide_set(rows, column, value):\n",
    "    split_function = None\n",
    "    if isinstance(value, int) or isinstance(value, float):\n",
    "        split_function = lambda row: row[column] >= value\n",
    "    else:\n",
    "        split_function = lambda row: row[column] == value\n",
    "\n",
    "    set1 = [row for row in rows if split_function(row)]\n",
    "    set2 = [row for row in rows if not split_function(row)]\n",
    "\n",
    "    return set1, set2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_tree(rows,depth):\n",
    "    if len(rows) == 0:\n",
    "        return 0\n",
    "    if depth == 0:\n",
    "        return 1\n",
    "\n",
    "    current_score = entropy(rows)\n",
    "    best_gain = 0.0\n",
    "    best_criteria = None\n",
    "    best_sets = None\n",
    "    column_count = len(rows[0]) - 1\n",
    "\n",
    "    for col in range(0, column_count):\n",
    "        column_values = {}\n",
    "        for row in rows:\n",
    "            column_values[row[col]] = 1\n",
    "        for value in column_values.keys():\n",
    "            set1, set2 = divide_set(rows, col, value)\n",
    "\n",
    "            p = float(len(set1)) / len(rows)\n",
    "            gain = current_score - p * entropy(set1) - (1 - p) * entropy(set2)\n",
    "            if gain > best_gain and len(set1) > 0 and len(set2) > 0:\n",
    "                best_gain = gain\n",
    "                best_criteria = (col, value)\n",
    "                best_sets = (set1, set2)\n",
    "\n",
    "    if best_gain > 0:\n",
    "        trueBranch = build_tree(best_sets[0], depth - 1)\n",
    "        falseBranch = build_tree(best_sets[1], depth - 1)\n",
    "        return {'col':best_criteria[0], 'value':best_criteria[1], 'results':None, 'tb':trueBranch, 'fb':falseBranch}\n",
    "    else:\n",
    "        return {'col':-1, 'value':None, 'results':unique_counts(rows), 'tb':None, 'fb':None}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tree_fit(rows, features_indexes):\n",
    "    if len(rows) < 1:\n",
    "        raise ValueError(\"Nao ha amostras suficientes no dataset de entrada.\")\n",
    "\n",
    "    rows = [get_features_subset(row, features_indexes) + [row[-1]] for row in rows]\n",
    "    \n",
    "    #return (build_tree(rows,-1), features_indexes)\n",
    "    return (build_tree(rows,-1), features_indexes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tree_classify(observation, tree):\n",
    "    if tree['results'] is not None:\n",
    "        return list(tree['results'].keys())[0]\n",
    "    else:\n",
    "        v = observation[tree['col']]\n",
    "        branch = None\n",
    "        if isinstance(v, int) or isinstance(v, float):\n",
    "            if v >= tree['value']:\n",
    "                branch = tree['tb']\n",
    "            else:\n",
    "                branch = tree['fb']\n",
    "        else:\n",
    "            if v == tree['value']:\n",
    "                branch = tree['tb']\n",
    "            else:\n",
    "                branch = tree['fb']\n",
    "        return tree_classify(observation, branch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tree_predict(features, features_indexes, tree):\n",
    "    if not all(i in range(len(features))\n",
    "            for i in features_indexes):\n",
    "        raise ValueError(\"As variaveis passadas nao batem com o conjunto utilizado para treino\")\n",
    "    features = get_features_subset(features, features_indexes)\n",
    "\n",
    "    return tree_classify(features, tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#def predict(trees, feature, features_indexes):\n",
    "def predict(trees, feature):\n",
    "    predictions = []\n",
    "\n",
    "    for tree in trees:\n",
    "        predictions.append(tree_predict(feature, tree[1], tree[0]))\n",
    "\n",
    "    return max(set(predictions), key=predictions.count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lendo o dataset e quebrando ele em 60 partições para que o Random Forest tenha 60 Árvores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = csv_to_rdd(\"../data/income.csv\").repartition(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Função que cria lista de listas em cada partição, pois precisamos de um RDD do tipo [indice da partição, ((linha 1),(linha 2)...)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dataset_tree(partition):\n",
    "    final_iterator = []\n",
    "    for sub_list in partition:\n",
    "        final_iterator.append(tuple(sub_list))\n",
    "    return iter(final_iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cirando um RDD de treino e um RDD de teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weights = [.7, .3]\n",
    "seed = 42\n",
    "train, test = data.randomSplit(weights, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22727"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_ = train.sample(False,0.5, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11370"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_withReplacement = train.union(train_).repartition(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34097"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_withReplacement.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_withReplacement.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Executando o Random Forest e calculando tempo de execução"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error rate: 18.954647142566607\n",
      "Execution took: 16.193546270000297 secs\n"
     ]
    }
   ],
   "source": [
    "from timeit import default_timer as timer\n",
    "\n",
    "#Inicio da execucao\n",
    "start = timer()\n",
    "\n",
    "#Criando o dataset que sera utilizado em cada uma das 60 arvores\n",
    "rdd_trees = train_withReplacement.mapPartitionsWithIndex(lambda index, part: (yield index, list(dataset_tree(part))))\n",
    "\n",
    "#seleciona aleatoriamente as variaveis que serao utilizadas\n",
    "#features_indexes = choose_random_features(rdd_trees.first()[1][0])\n",
    "\n",
    "#Calcula e retorna cada uma das 60 árvores\n",
    "#result_trees = rdd_trees.map(lambda x: tree_fit(x[1], features_indexes)).collect()\n",
    "result_trees = rdd_trees.map(lambda x: tree_fit(x[1], choose_random_features(x[1][0]))).collect()\n",
    "\n",
    "#Prediz a classe de cada linha do dataset de teste\n",
    "#my_predict = test.map(lambda x: (predict(result_trees, x[:-1], features_indexes), x[-1]))\n",
    "my_predict = test.map(lambda x: (predict(result_trees, x[:-1]), x[-1]))\n",
    "\n",
    "#Calcula taxa de erro\n",
    "total_count = my_predict.count()\n",
    "\n",
    "errors_count = my_predict.filter(lambda x: x[0]!= x[1]).count()\n",
    "\n",
    "print ('Error rate: ' + str(errors_count / total_count * 100))\n",
    "\n",
    "end = timer()\n",
    "\n",
    "print('Execution took: %s secs' % (end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
